{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric.transforms as T \n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='.', name='CiteSeer', transform=T.NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch_geometric.device('auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention Layer (GAT) as described in the paper `\"Graph Attention Networks\" <https://arxiv.org/pdf/1710.10903.pdf>`.\n",
    "\n",
    "        This operation can be mathematically described as:\n",
    "\n",
    "            e_ij = a(W h_i, W h_j)\n",
    "            α_ij = softmax_j(e_ij) = exp(e_ij) / Σ_k(exp(e_ik))     \n",
    "            h_i' = σ(Σ_j(α_ij W h_j))\n",
    "            \n",
    "            where h_i and h_j are the feature vectors of nodes i and j respectively, W is a learnable weight matrix,\n",
    "            a is an attention mechanism that computes the attention coefficients e_ij, and σ is an activation function.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, n_heads: int, concat: bool = False, dropout: float = 0.4, leaky_relu_slope: float = 0.2):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "\n",
    "        self.n_heads = n_heads # Number of attention heads\n",
    "        self.concat = concat # wether to concatenate the final attention heads\n",
    "        self.dropout = dropout # Dropout rate\n",
    "\n",
    "        if concat: # concatenating the attention heads\n",
    "            self.out_features = out_features # Number of output features per node\n",
    "            assert out_features % n_heads == 0 # Ensure that out_features is a multiple of n_heads\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else: # averaging output over the attention heads (Used in the main paper)\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        #  A shared linear transformation, parametrized by a weight matrix W is applied to every node\n",
    "        #  Initialize the weight matrix W \n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, self.n_hidden * n_heads)))\n",
    "\n",
    "        # Initialize the attention weights a\n",
    "        self.a = nn.Parameter(torch.empty(size=(n_heads, 2 * self.n_hidden, 1)))\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(leaky_relu_slope) # LeakyReLU activation function\n",
    "        self.softmax = nn.Softmax(dim=1) # softmax activation function to the attention coefficients\n",
    "\n",
    "        self.reset_parameters() # Reset the parameters\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Reinitialize learnable parameters.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        nn.init.xavier_normal_(self.a)\n",
    "    \n",
    "\n",
    "    def _get_attention_scores(self, h_transformed: torch.Tensor):\n",
    "        \"\"\"calculates the attention scores e_ij for all pairs of nodes (i, j) in the graph\n",
    "        in vectorized parallel form. for each pair of source and target nodes (i, j),\n",
    "        the attention score e_ij is computed as follows:\n",
    "\n",
    "            e_ij = LeakyReLU(a^T [Wh_i || Wh_j]) \n",
    "\n",
    "            where || denotes the concatenation operation, and a and W are the learnable parameters.\n",
    "\n",
    "        Args:\n",
    "            h_transformed (torch.Tensor): Transformed feature matrix with shape (n_nodes, n_heads, n_hidden),\n",
    "                where n_nodes is the number of nodes and out_features is the number of output features per node.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Attention score matrix with shape (n_heads, n_nodes, n_nodes), where n_nodes is the number of nodes.\n",
    "        \"\"\"\n",
    "        \n",
    "        source_scores = torch.matmul(h_transformed, self.a[:, :self.n_hidden, :])\n",
    "        target_scores = torch.matmul(h_transformed, self.a[:, self.n_hidden:, :])\n",
    "\n",
    "        # broadcast add \n",
    "        # (n_heads, n_nodes, 1) + (n_heads, 1, n_nodes) = (n_heads, n_nodes, n_nodes)\n",
    "        e = source_scores + target_scores.mT\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "    def forward(self,  h: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Performs a graph attention layer operation.\n",
    "\n",
    "        Args:\n",
    "            h (torch.Tensor): Input tensor representing node features.\n",
    "            adj_mat (torch.Tensor): Adjacency matrix representing graph structure.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after the graph convolution operation.\n",
    "        \"\"\"\n",
    "        n_nodes = h.shape[0]\n",
    "\n",
    "        # Apply linear transformation to node feature -> W h\n",
    "        # output shape (n_nodes, n_hidden * n_heads)\n",
    "        h_transformed = torch.mm(h, self.W)\n",
    "        h_transformed = F.dropout(h_transformed, self.dropout, training=self.training)\n",
    "\n",
    "        # splitting the heads by reshaping the tensor and putting heads dim first\n",
    "        # output shape (n_heads, n_nodes, n_hidden)\n",
    "        h_transformed = h_transformed.view(n_nodes, self.n_heads, self.n_hidden).permute(1, 0, 2)\n",
    "        \n",
    "        # getting the attention scores\n",
    "        # output shape (n_heads, n_nodes, n_nodes)\n",
    "        e = self._get_attention_scores(h_transformed)\n",
    "\n",
    "        # Set the attention score for non-existent edges to -9e15 (MASKING NON-EXISTENT EDGES)\n",
    "        connectivity_mask = -9e16 * torch.ones_like(e)\n",
    "        e = torch.where(adj_mat > 0, e, connectivity_mask) # masked attention scores\n",
    "        \n",
    "        # attention coefficients are computed as a softmax over the rows\n",
    "        # for each column j in the attention score matrix e\n",
    "        attention = F.softmax(e, dim=-1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "\n",
    "        # final node embeddings are computed as a weighted average of the features of its neighbors\n",
    "        h_prime = torch.matmul(attention, h_transformed)\n",
    "\n",
    "        # concatenating/averaging the attention heads\n",
    "        # output shape (n_nodes, out_features)\n",
    "        if self.concat:\n",
    "            h_prime = h_prime.permute(1, 0, 2).contiguous().view(n_nodes, self.out_features)\n",
    "        else:\n",
    "            h_prime = h_prime.mean(dim=0)\n",
    "\n",
    "        return h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention Network (GAT) as described in the paper `\"Graph Attention Networks\" <https://arxiv.org/pdf/1710.10903.pdf>`.\n",
    "    Consists of a 2-layer stack of Graph Attention Layers (GATs). The fist GAT Layer is followed by an ELU activation.\n",
    "    And the second (final) layer is a GAT layer with a single attention head and softmax activation function. \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        in_features,\n",
    "        n_hidden,\n",
    "        n_heads,\n",
    "        num_classes,\n",
    "        concat=False,\n",
    "        dropout=0.4,\n",
    "        leaky_relu_slope=0.2):\n",
    "        \"\"\" Initializes the GAT model. \n",
    "\n",
    "        Args:\n",
    "            in_features (int): number of input features per node.\n",
    "            n_hidden (int): output size of the first Graph Attention Layer.\n",
    "            n_heads (int): number of attention heads in the first Graph Attention Layer.\n",
    "            num_classes (int): number of classes to predict for each node.\n",
    "            concat (bool, optional): Wether to concatinate attention heads or take an average over them for the\n",
    "                output of the first Graph Attention Layer. Defaults to False.\n",
    "            dropout (float, optional): dropout rate. Defaults to 0.4.\n",
    "            leaky_relu_slope (float, optional): alpha (slope) of the leaky relu activation. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        # Define the Graph Attention layers\n",
    "        self.gat1 = GraphAttentionLayer(\n",
    "            in_features=in_features, out_features=n_hidden, n_heads=n_heads,\n",
    "            concat=concat, dropout=dropout, leaky_relu_slope=leaky_relu_slope\n",
    "            )\n",
    "        \n",
    "        self.gat2 = GraphAttentionLayer(\n",
    "            in_features=n_hidden, out_features=num_classes, n_heads=1,\n",
    "            concat=False, dropout=dropout, leaky_relu_slope=leaky_relu_slope\n",
    "            )\n",
    "        \n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor , adj_mat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            input_tensor (torch.Tensor): Input tensor representing node features.\n",
    "            adj_mat (torch.Tensor): Adjacency matrix representing graph structure.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after the forward pass.\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply the first Graph Attention layer\n",
    "        x = self.gat1(input_tensor, adj_mat)\n",
    "        x = F.elu(x) # Apply ELU activation function to the output of the first layer\n",
    "\n",
    "        # Apply the second Graph Attention layer\n",
    "        x = self.gat2(x, adj_mat)\n",
    "\n",
    "        return F.log_softmax(x, dim=1) # Apply log softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "lr = 0.005\n",
    "l2 = 5e-4\n",
    "dropout = 0.6\n",
    "num_heads = 8\n",
    "hidden_dim = 64\n",
    "val_every = 20\n",
    "num_classes = dataset.num_classes\n",
    "dataset = dataset[0].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT(\n",
    "        in_features=dataset.num_features,          # Number of input features per node  \n",
    "        n_hidden=hidden_dim,               # Output size of the first Graph Attention Layer\n",
    "        n_heads=num_heads,                 # Number of attention heads in the first Graph Attention Layer\n",
    "        num_classes=num_classes,    # Number of classes to predict for each node\n",
    "        concat=False,               # Wether to concatinate attention heads\n",
    "        dropout=dropout,                 # Dropout rate\n",
    "        leaky_relu_slope=0.2                    # Alpha (slope) of the leaky relu activation\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, criterion, input, target, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(*input)\n",
    "        output, target = output[mask], target[mask]\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        acc = (output.argmax(dim=1) == target).float().sum() / len(target)\n",
    "    return loss.item(), acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(epoch, model, optimizer, criterion, input, target, mask_train, mask_val, print_every=10):\n",
    "    start_t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(*input) \n",
    "    loss = criterion(output[mask_train], target[mask_train]) # Compute the loss using the training mask\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model performance on training and validation sets\n",
    "    loss_train, acc_train = test(model, criterion, input, target, mask_train)\n",
    "    loss_val, acc_val = test(model, criterion, input, target, mask_val)\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        # Print the training progress at specified intervals\n",
    "        print(f'Epoch: {epoch:04d} ({(time.time() - start_t):.4f}s) loss_train: {loss_train:.4f} acc_train: {acc_train:.4f} loss_val: {loss_val:.4f} acc_val: {acc_val:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, edge_index = dataset.x, dataset.y, dataset.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (9104) must match the size of tensor b (3327) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      4\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     train_acc, val_acc, tmp_test_acc \u001b[38;5;241m=\u001b[39m test(dataset)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_acc \u001b[38;5;241m>\u001b[39m best_val_acc:\n",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 4\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[1;32m      6\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Graph Codes/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Graph Codes/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[20], line 55\u001b[0m, in \u001b[0;36mGAT.forward\u001b[0;34m(self, input_tensor, adj_mat)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mPerforms a forward pass through the network.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Output tensor after the forward pass.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Apply the first Graph Attention layer\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_mat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(x) \u001b[38;5;66;03m# Apply ELU activation function to the output of the first layer\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Apply the second Graph Attention layer\u001b[39;00m\n",
      "File \u001b[0;32m~/Graph Codes/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Graph Codes/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[19], line 103\u001b[0m, in \u001b[0;36mGraphAttentionLayer.forward\u001b[0;34m(self, h, adj_mat)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Set the attention score for non-existent edges to -9e15 (MASKING NON-EXISTENT EDGES)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m connectivity_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9e16\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(e)\n\u001b[0;32m--> 103\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj_mat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnectivity_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# masked attention scores\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# attention coefficients are computed as a softmax over the rows\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# for each column j in the attention score matrix e\u001b[39;00m\n\u001b[1;32m    107\u001b[0m attention \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(e, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (9104) must match the size of tensor b (3327) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "best_val_acc = test_acc = 0\n",
    "times = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start = time.time()\n",
    "    \n",
    "    loss = train(dataset)\n",
    "    train_acc, val_acc, tmp_test_acc = test(dataset)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    print(f'Epoch = {epoch}, Loss = {loss}, Train Accuracy = {train_acc}, Val Accuracy ={val_acc}, Test Accuracy = {test_acc}')\n",
    "    times.append(time.time() - start)\n",
    "print(f'Median time per epoch: {torch.tensor(times).median():.4f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
